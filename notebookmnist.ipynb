{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from __future__ import print_function, division\nimport torch.utils.data as data\nfrom PIL import Image\nimport os\nimport os.path\nimport errno\nimport numpy as np\nimport torch\nimport codecs\nimport torch.nn as nn\nimport copy\nimport time\nfrom sklearn.cluster import KMeans\nimport numpy as np\nimport sklearn.metrics\nfrom torchvision import transforms\nimport argparse\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torchvision import datasets, models, transforms\nimport math\nimport fnmatch\nfrom torch.utils.tensorboard import SummaryWriter\nimport torch.nn.functional as F\nimport pandas as pd\nfrom matplotlib import pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MNIST(data.Dataset):\n    \"\"\"`MNIST <http://yann.lecun.com/exdb/mnist/>`_ Dataset.\n\n    Args:\n        root (string): Root directory of dataset where ``processed/training.pt``\n            and  ``processed/test.pt`` exist.\n        train (bool, optional): If True, creates dataset from ``training.pt``,\n            otherwise from ``test.pt``.\n        download (bool, optional): If true, downloads the dataset from the internet and\n            puts it in root directory. If dataset is already downloaded, it is not\n            downloaded again.\n        transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n    \"\"\"\n    urls = [\n        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n    ]\n    raw_folder = 'raw'\n    processed_folder = 'processed'\n    training_file = 'training.pt'\n    test_file = 'test.pt'\n\n    def __init__(self, root, train=True, transform=None, target_transform=None, download=False, small=False, full=False):\n        self.root = os.path.expanduser(root)\n        self.transform = transform\n        self.target_transform = target_transform\n        self.train = train  # training set or test set\n        self.full = full\n\n        if full:\n            self.train = True\n\n        if download:\n            self.download()\n\n        if not self._check_exists():\n            raise RuntimeError('Dataset not found.' +\n                               ' You can use download=True to download it')\n\n        self.train_data, self.train_labels = torch.load(os.path.join(self.root, self.processed_folder, self.training_file))\n        self.test_data, self.test_labels = torch.load(os.path.join(self.root, self.processed_folder, self.test_file))\n\n        if full:\n            self.train_data = np.concatenate((self.train_data, self.test_data), axis=0)\n            self.train_labels = np.concatenate((self.train_labels, self.test_labels), axis=0)\n\n        if small:\n            self.train_data = self.train_data[0:1400]\n            self.train_labels = self.train_labels[0:1400]\n            if not full:\n                self.train_data = self.train_data[0:1200]\n                self.train_labels = self.train_labels[0:1200]\n            self.test_data = self.test_data[0:200]\n            self.test_labels = self.test_labels[0:200]\n\n    def __getitem__(self, index):\n        \"\"\"\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (image, target) where target is index of the target class.\n        \"\"\"\n        if self.train:\n            img, target = self.train_data[index], self.train_labels[index]\n        else:\n            img, target = self.test_data[index], self.test_labels[index]\n\n        # doing this so that it is consistent with all other datasets\n        # to return a PIL Image\n        if self.full:\n            img = Image.fromarray(img, mode='L')\n        else:\n            img = Image.fromarray(img.numpy(), mode='L')\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target\n\n    def __len__(self):\n        if self.train:\n            return len(self.train_data)\n        else:\n            return len(self.test_data)\n\n    def _check_exists(self):\n        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n\n    def download(self):\n        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n        from six.moves import urllib\n        import gzip\n\n        if self._check_exists():\n            return\n\n        # download files\n        try:\n            os.makedirs(os.path.join(self.root, self.raw_folder))\n            os.makedirs(os.path.join(self.root, self.processed_folder))\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                pass\n            else:\n                raise\n\n        for url in self.urls:\n            print('Downloading ' + url)\n            data = urllib.request.urlopen(url)\n            filename = url.rpartition('/')[2]\n            file_path = os.path.join(self.root, self.raw_folder, filename)\n            with open(file_path, 'wb') as f:\n                f.write(data.read())\n            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n                    gzip.GzipFile(file_path) as zip_f:\n                out_f.write(zip_f.read())\n            os.unlink(file_path)\n\n        # process and save as torch files\n        print('Processing...')\n\n        training_set = (\n            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n        )\n        test_set = (\n            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n        )\n        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n            torch.save(training_set, f)\n        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n            torch.save(test_set, f)\n\n        print('Done!')\n\n    def __repr__(self):\n        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n        tmp = 'train' if self.train is True else 'test'\n        fmt_str += '    Split: {}\\n'.format(tmp)\n        fmt_str += '    Root Location: {}\\n'.format(self.root)\n        tmp = '    Transforms (if any): '\n        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n        tmp = '    Target Transforms (if any): '\n        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n        return fmt_str\n\n\nclass FashionMNIST(MNIST):\n    \"\"\"`Fashion-MNIST <https://github.com/zalandoresearch/fashion-mnist>`_ Dataset.\n\n    Args:\n        root (string): Root directory of dataset where ``processed/training.pt``\n            and  ``processed/test.pt`` exist.\n        train (bool, optional): If True, creates dataset from ``training.pt``,\n            otherwise from ``test.pt``.\n        download (bool, optional): If true, downloads the dataset from the internet and\n            puts it in root directory. If dataset is already downloaded, it is not\n            downloaded again.\n        transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n    \"\"\"\n    urls = [\n        'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz',\n        'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz',\n        'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz',\n        'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz',\n    ]\n\n\nclass EMNIST(MNIST):\n    \"\"\"`EMNIST <https://www.nist.gov/itl/iad/image-group/emnist-dataset/>`_ Dataset.\n\n    Args:\n        root (string): Root directory of dataset where ``processed/training.pt``\n            and  ``processed/test.pt`` exist.\n        split (string): The dataset has 6 different splits: ``byclass``, ``bymerge``,\n            ``balanced``, ``letters``, ``digits`` and ``mnist``. This argument specifies\n            which one to use.\n        train (bool, optional): If True, creates dataset from ``training.pt``,\n            otherwise from ``test.pt``.\n        download (bool, optional): If true, downloads the dataset from the internet and\n            puts it in root directory. If dataset is already downloaded, it is not\n            downloaded again.\n        transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n    \"\"\"\n    url = 'http://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip'\n    splits = ('byclass', 'bymerge', 'balanced', 'letters', 'digits', 'mnist')\n\n    def __init__(self, root, split, **kwargs):\n        if split not in self.splits:\n            raise ValueError('Split \"{}\" not found. Valid splits are: {}'.format(\n                split, ', '.join(self.splits),\n            ))\n        self.split = split\n        self.training_file = self._training_file(split)\n        self.test_file = self._test_file(split)\n        super(EMNIST, self).__init__(root, **kwargs)\n\n    def _training_file(self, split):\n        return 'training_{}.pt'.format(split)\n\n    def _test_file(self, split):\n        return 'test_{}.pt'.format(split)\n\n    def download(self):\n        \"\"\"Download the EMNIST data if it doesn't exist in processed_folder already.\"\"\"\n        from six.moves import urllib\n        import gzip\n        import shutil\n        import zipfile\n\n        if self._check_exists():\n            return\n\n        # download files\n        try:\n            os.makedirs(os.path.join(self.root, self.raw_folder))\n            os.makedirs(os.path.join(self.root, self.processed_folder))\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                pass\n            else:\n                raise\n\n        print('Downloading ' + self.url)\n        data = urllib.request.urlopen(self.url)\n        filename = self.url.rpartition('/')[2]\n        raw_folder = os.path.join(self.root, self.raw_folder)\n        file_path = os.path.join(raw_folder, filename)\n        with open(file_path, 'wb') as f:\n            f.write(data.read())\n\n        print('Extracting zip archive')\n        with zipfile.ZipFile(file_path) as zip_f:\n            zip_f.extractall(raw_folder)\n        os.unlink(file_path)\n        gzip_folder = os.path.join(raw_folder, 'gzip')\n        for gzip_file in os.listdir(gzip_folder):\n            if gzip_file.endswith('.gz'):\n                print('Extracting ' + gzip_file)\n                with open(os.path.join(raw_folder, gzip_file.replace('.gz', '')), 'wb') as out_f, \\\n                        gzip.GzipFile(os.path.join(gzip_folder, gzip_file)) as zip_f:\n                    out_f.write(zip_f.read())\n        shutil.rmtree(gzip_folder)\n\n        # process and save as torch files\n        for split in self.splits:\n            print('Processing ' + split)\n            training_set = (\n                read_image_file(os.path.join(raw_folder, 'emnist-{}-train-images-idx3-ubyte'.format(split))),\n                read_label_file(os.path.join(raw_folder, 'emnist-{}-train-labels-idx1-ubyte'.format(split)))\n            )\n            test_set = (\n                read_image_file(os.path.join(raw_folder, 'emnist-{}-test-images-idx3-ubyte'.format(split))),\n                read_label_file(os.path.join(raw_folder, 'emnist-{}-test-labels-idx1-ubyte'.format(split)))\n            )\n            with open(os.path.join(self.root, self.processed_folder, self._training_file(split)), 'wb') as f:\n                torch.save(training_set, f)\n            with open(os.path.join(self.root, self.processed_folder, self._test_file(split)), 'wb') as f:\n                torch.save(test_set, f)\n\n        print('Done!')\n\n\ndef get_int(b):\n    return int(codecs.encode(b, 'hex'), 16)\n\n\ndef read_label_file(path):\n    with open(path, 'rb') as f:\n        data = f.read()\n        assert get_int(data[:4]) == 2049\n        length = get_int(data[4:8])\n        parsed = np.frombuffer(data, dtype=np.uint8, offset=8)\n        return torch.from_numpy(parsed).view(length).long()\n\n\ndef read_image_file(path):\n    with open(path, 'rb') as f:\n        data = f.read()\n        assert get_int(data[:4]) == 2051\n        length = get_int(data[4:8])\n        num_rows = get_int(data[8:12])\n        num_cols = get_int(data[12:16])\n        images = []\n        parsed = np.frombuffer(data, dtype=np.uint8, offset=16)\n        return torch.from_numpy(parsed).view(length, num_rows, num_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ClusterlingLayer(nn.Module):\n    def __init__(self, in_features=10, out_features=10, alpha=1.0):\n        super(ClusterlingLayer, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.alpha = alpha\n        self.weight = nn.Parameter(torch.Tensor(self.out_features, self.in_features))\n        self.weight = nn.init.xavier_uniform_(self.weight)\n\n    def forward(self, x):\n        x = x.unsqueeze(1) - self.weight\n        x = torch.mul(x, x)\n        x = torch.sum(x, dim=2)\n        x = 1.0 + (x / self.alpha)\n        x = 1.0 / x\n        x = x ** ((self.alpha +1.0) / 2.0)\n        x = torch.t(x) / torch.sum(x, dim=1)\n        x = torch.t(x)\n        return x\n\n    def extra_repr(self):\n        return 'in_features={}, out_features={}, alpha={}'.format(\n            self.in_features, self.out_features, self.alpha\n        )\n\n    def set_weight(self, tensor):\n        self.weight = nn.Parameter(tensor)\n\n\n# Convolutional autoencoder directly from DCEC article\nclass CAE_3(nn.Module):\n    def __init__(self, input_shape=[128,128,3], num_clusters=10, filters=[32, 64, 128], leaky=True, neg_slope=0.01, activations=False, bias=True):\n        super(CAE_3, self).__init__()\n        self.activations = activations\n        # bias = True\n        self.pretrained = False\n        self.num_clusters = num_clusters\n        self.input_shape = input_shape\n        self.filters = filters\n        self.conv1 = nn.Conv2d(input_shape[2], filters[0], 5, stride=2, padding=2, bias=bias)\n        if leaky:\n            self.relu = nn.LeakyReLU(negative_slope=neg_slope)\n        else:\n            self.relu = nn.ReLU(inplace=False)\n        self.conv2 = nn.Conv2d(filters[0], filters[1], 5, stride=2, padding=2, bias=bias)\n        self.conv3 = nn.Conv2d(filters[1], filters[2], 3, stride=2, padding=0, bias=bias)\n        lin_features_len = ((input_shape[0]//2//2-1) // 2) * ((input_shape[0]//2//2-1) // 2) * filters[2]\n        self.embedding = nn.Linear(lin_features_len, num_clusters, bias=bias)\n        self.deembedding = nn.Linear(num_clusters, lin_features_len, bias=bias)\n        out_pad = 1 if input_shape[0] // 2 // 2 % 2 == 0 else 0\n        self.deconv3 = nn.ConvTranspose2d(filters[2], filters[1], 3, stride=2, padding=0, output_padding=out_pad, bias=bias)\n        out_pad = 1 if input_shape[0] // 2 % 2 == 0 else 0\n        self.deconv2 = nn.ConvTranspose2d(filters[1], filters[0], 5, stride=2, padding=2, output_padding=out_pad, bias=bias)\n        out_pad = 1 if input_shape[0] % 2 == 0 else 0\n        self.deconv1 = nn.ConvTranspose2d(filters[0], input_shape[2], 5, stride=2, padding=2, output_padding=out_pad, bias=bias)\n        self.clustering = ClusterlingLayer(num_clusters, num_clusters)\n        # ReLU copies for graph representation in tensorboard\n        self.relu1_1 = copy.deepcopy(self.relu)\n        self.relu2_1 = copy.deepcopy(self.relu)\n        self.relu3_1 = copy.deepcopy(self.relu)\n        self.relu1_2 = copy.deepcopy(self.relu)\n        self.relu2_2 = copy.deepcopy(self.relu)\n        self.relu3_2 = copy.deepcopy(self.relu)\n        self.sig = nn.Sigmoid()\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu1_1(x)\n        x = self.conv2(x)\n        x = self.relu2_1(x)\n        x = self.conv3(x)\n        if self.activations:\n            x = self.sig(x)\n        else:\n            x = self.relu3_1(x)\n        x = x.view(x.size(0), -1)\n        x = self.embedding(x)\n        extra_out = x\n        clustering_out = self.clustering(x)\n        x = self.deembedding(x)\n        x = self.relu1_2(x)\n        x = x.view(x.size(0), self.filters[2], ((self.input_shape[0]//2//2-1) // 2), ((self.input_shape[0]//2//2-1) // 2))\n        x = self.deconv3(x)\n        x = self.relu2_2(x)\n        x = self.deconv2(x)\n        x = self.relu3_2(x)\n        x = self.deconv1(x)\n        if self.activations:\n            x = self.tanh(x)\n        return x, clustering_out, extra_out\n\n\n# Convolutional autoencoder from DCEC article with Batch Norms and Leaky ReLUs\nclass CAE_bn3(nn.Module):\n    def __init__(self, input_shape=[128,128,3], num_clusters=10, filters=[32, 64, 128], leaky=True, neg_slope=0.01, activations=False, bias=True):\n        super(CAE_bn3, self).__init__()\n        self.activations=activations\n        self.pretrained = False\n        self.num_clusters = num_clusters\n        self.input_shape = input_shape\n        self.filters = filters\n        self.conv1 = nn.Conv2d(input_shape[2], filters[0], 5, stride=2, padding=2, bias=bias)\n        self.bn1_1 = nn.BatchNorm2d(filters[0])\n        if leaky:\n            self.relu = nn.LeakyReLU(negative_slope=neg_slope)\n        else:\n            self.relu = nn.ReLU(inplace=False)\n        self.conv2 = nn.Conv2d(filters[0], filters[1], 5, stride=2, padding=2, bias=bias)\n        self.bn2_1 = nn.BatchNorm2d(filters[1])\n        self.conv3 = nn.Conv2d(filters[1], filters[2], 3, stride=2, padding=0, bias=bias)\n        lin_features_len = ((input_shape[0]//2//2-1) // 2) * ((input_shape[0]//2//2-1) // 2) * filters[2]\n        self.embedding = nn.Linear(lin_features_len, num_clusters, bias=bias)\n        self.deembedding = nn.Linear(num_clusters, lin_features_len, bias=bias)\n        out_pad = 1 if input_shape[0] // 2 // 2 % 2 == 0 else 0\n        self.deconv3 = nn.ConvTranspose2d(filters[2], filters[1], 3, stride=2, padding=0, output_padding=out_pad, bias=bias)\n        out_pad = 1 if input_shape[0] // 2 % 2 == 0 else 0\n        self.bn3_2 = nn.BatchNorm2d(filters[1])\n        self.deconv2 = nn.ConvTranspose2d(filters[1], filters[0], 5, stride=2, padding=2, output_padding=out_pad, bias=bias)\n        out_pad = 1 if input_shape[0] % 2 == 0 else 0\n        self.bn2_2 = nn.BatchNorm2d(filters[0])\n        self.deconv1 = nn.ConvTranspose2d(filters[0], input_shape[2], 5, stride=2, padding=2, output_padding=out_pad, bias=bias)\n        self.clustering = ClusterlingLayer(num_clusters, num_clusters)\n        # ReLU copies for graph representation in tensorboard\n        self.relu1_1 = copy.deepcopy(self.relu)\n        self.relu2_1 = copy.deepcopy(self.relu)\n        self.relu3_1 = copy.deepcopy(self.relu)\n        self.relu1_2 = copy.deepcopy(self.relu)\n        self.relu2_2 = copy.deepcopy(self.relu)\n        self.relu3_2 = copy.deepcopy(self.relu)\n        self.sig = nn.Sigmoid()\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu1_1(x)\n        x1 = self.bn1_1(x)\n        x2 = self.conv2(x1)\n        x2 = self.relu2_1(x2)\n        x2 = self.bn2_1(x2)\n        x = self.conv3(x2)\n        if self.activations:\n            x = self.sig(x)\n        else:\n            x = self.relu3_1(x)\n        x = x.view(x.size(0), -1)\n        x = self.embedding(x)\n        extra_out = x\n        clustering_out = self.clustering(x)\n        x = self.deembedding(x)\n        x = self.relu1_2(x)\n        x = x.view(x.size(0), self.filters[2], ((self.input_shape[0]//2//2-1) // 2), ((self.input_shape[0]//2//2-1) // 2))\n        x = self.deconv3(x)\n        x = self.relu2_2(x)\n        x = self.bn3_2(x)\n        x = self.deconv2(x)\n        x = self.relu3_2(x)\n        x = self.bn2_2(x)\n        x = self.deconv1(x)\n        if self.activations:\n            x = self.tanh(x)\n        return x, clustering_out, extra_out, [x1,x2]\n\n\n# Convolutional autoencoder with 4 convolutional blocks\nclass CAE_4(nn.Module):\n    def __init__(self, input_shape=[128,128,3], num_clusters=10, filters=[32, 64, 128, 256], leaky=True, neg_slope=0.01, activations=False, bias=True):\n        super(CAE_4, self).__init__()\n        self.activations = activations\n        self.pretrained = False\n        self.num_clusters = num_clusters\n        self.input_shape = input_shape\n        self.filters = filters\n        if leaky:\n            self.relu = nn.LeakyReLU(negative_slope=neg_slope)\n        else:\n            self.relu = nn.ReLU(inplace=False)\n\n        self.conv1 = nn.Conv2d(input_shape[2], filters[0], 5, stride=2, padding=2, bias=bias)\n        self.conv2 = nn.Conv2d(filters[0], filters[1], 5, stride=2, padding=2, bias=bias)\n        self.conv3 = nn.Conv2d(filters[1], filters[2], 5, stride=2, padding=2, bias=bias)\n        self.conv4 = nn.Conv2d(filters[2], filters[3], 3, stride=2, padding=0, bias=bias)\n\n        lin_features_len = ((input_shape[0] // 2 // 2 // 2 - 1) // 2) * ((input_shape[0] // 2 // 2 // 2 - 1) // 2) * \\\n                           filters[3]\n        self.embedding = nn.Linear(lin_features_len, num_clusters, bias=bias)\n        self.deembedding = nn.Linear(num_clusters, lin_features_len, bias=bias)\n        out_pad = 1 if input_shape[0] // 2 // 2 // 2 % 2 == 0 else 0\n        self.deconv4 = nn.ConvTranspose2d(filters[3], filters[2], 3, stride=2, padding=0, output_padding=out_pad,\n                                          bias=bias)\n        out_pad = 1 if input_shape[0] // 2 // 2 % 2 == 0 else 0\n        self.deconv3 = nn.ConvTranspose2d(filters[2], filters[1], 5, stride=2, padding=2, output_padding=out_pad,\n                                          bias=bias)\n        out_pad = 1 if input_shape[0] // 2 % 2 == 0 else 0\n        self.deconv2 = nn.ConvTranspose2d(filters[1], filters[0], 5, stride=2, padding=2, output_padding=out_pad,\n                                          bias=bias)\n        out_pad = 1 if input_shape[0] % 2 == 0 else 0\n        self.deconv1 = nn.ConvTranspose2d(filters[0], input_shape[2], 5, stride=2, padding=2, output_padding=out_pad,\n                                          bias=bias)\n        self.clustering = ClusterlingLayer(num_clusters, num_clusters)\n        # ReLU copies for graph representation in tensorboard\n        self.relu1_1 = copy.deepcopy(self.relu)\n        self.relu2_1 = copy.deepcopy(self.relu)\n        self.relu3_1 = copy.deepcopy(self.relu)\n        self.relu4_1 = copy.deepcopy(self.relu)\n        self.relu1_2 = copy.deepcopy(self.relu)\n        self.relu2_2 = copy.deepcopy(self.relu)\n        self.relu3_2 = copy.deepcopy(self.relu)\n        self.relu4_2 = copy.deepcopy(self.relu)\n        self.sig = nn.Sigmoid()\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu1_1(x)\n        x = self.conv2(x)\n        x = self.relu2_1(x)\n        x = self.conv3(x)\n        x = self.relu3_1(x)\n        x = self.conv4(x)\n        if self.activations:\n            x = self.sig(x)\n        else:\n            x = self.relu4_1(x)\n        x = x.view(x.size(0), -1)\n        x = self.embedding(x)\n        extra_out = x\n        clustering_out = self.clustering(x)\n        x = self.deembedding(x)\n        x = self.relu4_2(x)\n        x = x.view(x.size(0), self.filters[3], ((self.input_shape[0]//2//2//2-1) // 2), ((self.input_shape[0]//2//2//2-1) // 2))\n        x = self.deconv4(x)\n        x = self.relu3_2(x)\n        x = self.deconv3(x)\n        x = self.relu2_2(x)\n        x = self.deconv2(x)\n        x = self.relu1_2(x)\n        x = self.deconv1(x)\n        if self.activations:\n            x = self.tanh(x)\n        return x, clustering_out, extra_out\n\n# Convolutional autoencoder with 4 convolutional blocks (BN version)\nclass CAE_bn4(nn.Module):\n    def __init__(self, input_shape=[128,128,3], num_clusters=10, filters=[32, 64, 128, 256], leaky=True, neg_slope=0.01, activations=False, bias=True):\n        super(CAE_bn4, self).__init__()\n        self.activations = activations\n        self.pretrained = False\n        self.num_clusters = num_clusters\n        self.input_shape = input_shape\n        self.filters = filters\n        if leaky:\n            self.relu = nn.LeakyReLU(negative_slope=neg_slope)\n        else:\n            self.relu = nn.ReLU(inplace=False)\n\n        self.conv1 = nn.Conv2d(input_shape[2], filters[0], 5, stride=2, padding=2, bias=bias)\n        self.bn1_1 = nn.BatchNorm2d(filters[0])\n        self.conv2 = nn.Conv2d(filters[0], filters[1], 5, stride=2, padding=2, bias=bias)\n        self.bn2_1 = nn.BatchNorm2d(filters[1])\n        self.conv3 = nn.Conv2d(filters[1], filters[2], 5, stride=2, padding=2, bias=bias)\n        self.bn3_1 = nn.BatchNorm2d(filters[2])\n        self.conv4 = nn.Conv2d(filters[2], filters[3], 3, stride=2, padding=0, bias=bias)\n\n        lin_features_len = ((input_shape[0] // 2 // 2 // 2 - 1) // 2) * ((input_shape[0] // 2 // 2 // 2 - 1) // 2) * \\\n                           filters[3]\n        self.embedding = nn.Linear(lin_features_len, num_clusters, bias=bias)\n        self.deembedding = nn.Linear(num_clusters, lin_features_len, bias=bias)\n        out_pad = 1 if input_shape[0] // 2 // 2 // 2 % 2 == 0 else 0\n        self.deconv4 = nn.ConvTranspose2d(filters[3], filters[2], 3, stride=2, padding=0, output_padding=out_pad,\n                                          bias=bias)\n        self.bn4_2 = nn.BatchNorm2d(filters[2])\n        out_pad = 1 if input_shape[0] // 2 // 2 % 2 == 0 else 0\n        self.deconv3 = nn.ConvTranspose2d(filters[2], filters[1], 5, stride=2, padding=2, output_padding=out_pad,\n                                          bias=bias)\n        self.bn3_2 = nn.BatchNorm2d(filters[1])\n        out_pad = 1 if input_shape[0] // 2 % 2 == 0 else 0\n        self.deconv2 = nn.ConvTranspose2d(filters[1], filters[0], 5, stride=2, padding=2, output_padding=out_pad,\n                                          bias=bias)\n        self.bn2_2 = nn.BatchNorm2d(filters[0])\n        out_pad = 1 if input_shape[0] % 2 == 0 else 0\n        self.deconv1 = nn.ConvTranspose2d(filters[0], input_shape[2], 5, stride=2, padding=2, output_padding=out_pad,\n                                          bias=bias)\n        self.clustering = ClusterlingLayer(num_clusters, num_clusters)\n        # ReLU copies for graph representation in tensorboard\n        self.relu1_1 = copy.deepcopy(self.relu)\n        self.relu2_1 = copy.deepcopy(self.relu)\n        self.relu3_1 = copy.deepcopy(self.relu)\n        self.relu4_1 = copy.deepcopy(self.relu)\n        self.relu1_2 = copy.deepcopy(self.relu)\n        self.relu2_2 = copy.deepcopy(self.relu)\n        self.relu3_2 = copy.deepcopy(self.relu)\n        self.relu4_2 = copy.deepcopy(self.relu)\n        self.sig = nn.Sigmoid()\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu1_1(x)\n        x1 = self.bn1_1(x)\n        x2 = self.conv2(x1)\n        x2 = self.relu2_1(x2)\n        x2 = self.bn2_1(x2)\n        x3 = self.conv3(x2)\n        x3 = self.relu3_1(x3)\n        x3 = self.bn3_1(x3)\n        x = self.conv4(x3)\n        if self.activations:\n            x = self.sig(x)\n        else:\n            x = self.relu4_1(x)\n        x = x.view(x.size(0), -1)\n        x = self.embedding(x)\n        extra_out = x\n        clustering_out = self.clustering(x)\n        x = self.deembedding(x)\n        x = self.relu4_2(x)\n        x = x.view(x.size(0), self.filters[3], ((self.input_shape[0]//2//2//2-1) // 2), ((self.input_shape[0]//2//2//2-1) // 2))\n        x = self.deconv4(x)\n        x = self.relu3_2(x)\n        x = self.bn4_2(x)\n        x = self.deconv3(x)\n        x = self.relu2_2(x)\n        x = self.bn3_2(x)\n        x = self.deconv2(x)\n        x = self.relu1_2(x)\n        x = self.bn2_2(x)\n        x = self.deconv1(x)\n        if self.activations:\n            x = self.tanh(x)\n        return x, clustering_out, extra_out,[x1,x2,x3]\n\n\n# Convolutional autoencoder with 5 convolutional blocks\nclass CAE_5(nn.Module):\n    def __init__(self, input_shape=[128,128,3], num_clusters=10, filters=[32, 64, 128, 256, 512], leaky=True, neg_slope=0.01, activations=False, bias=True):\n        super(CAE_5, self).__init__()\n        self.activations = activations\n        self.pretrained = False\n        self.num_clusters = num_clusters\n        self.input_shape = input_shape\n        self.filters = filters\n        self.relu = nn.ReLU(inplace=False)\n        if leaky:\n            self.relu = nn.LeakyReLU(negative_slope=neg_slope)\n        else:\n            self.relu = nn.ReLU(inplace=False)\n\n        self.conv1 = nn.Conv2d(input_shape[2], filters[0], 5, stride=2, padding=2, bias=bias)\n        self.conv2 = nn.Conv2d(filters[0], filters[1], 5, stride=2, padding=2, bias=bias)\n        self.conv3 = nn.Conv2d(filters[1], filters[2], 5, stride=2, padding=2, bias=bias)\n        self.conv4 = nn.Conv2d(filters[2], filters[3], 5, stride=2, padding=2, bias=bias)\n        self.conv5 = nn.Conv2d(filters[3], filters[4], 3, stride=2, padding=0, bias=bias)\n\n        lin_features_len = ((input_shape[0] // 2 // 2 // 2 // 2 - 1) // 2) * (\n                    (input_shape[0] // 2 // 2 // 2 // 2 - 1) // 2) * filters[4]\n        self.embedding = nn.Linear(lin_features_len, num_clusters, bias=bias)\n        self.deembedding = nn.Linear(num_clusters, lin_features_len, bias=bias)\n        out_pad = 1 if input_shape[0] // 2 // 2 // 2 // 2 % 2 == 0 else 0\n        self.deconv5 = nn.ConvTranspose2d(filters[4], filters[3], 3, stride=2, padding=0, output_padding=out_pad,\n                                          bias=bias)\n        out_pad = 1 if input_shape[0] // 2 // 2 // 2 % 2 == 0 else 0\n        self.deconv4 = nn.ConvTranspose2d(filters[3], filters[2], 5, stride=2, padding=2, output_padding=out_pad,\n                                          bias=bias)\n        out_pad = 1 if input_shape[0] // 2 // 2 % 2 == 0 else 0\n        self.deconv3 = nn.ConvTranspose2d(filters[2], filters[1], 5, stride=2, padding=2, output_padding=out_pad,\n                                          bias=bias)\n        out_pad = 1 if input_shape[0] // 2 % 2 == 0 else 0\n        self.deconv2 = nn.ConvTranspose2d(filters[1], filters[0], 5, stride=2, padding=2, output_padding=out_pad,\n                                          bias=bias)\n        out_pad = 1 if input_shape[0] % 2 == 0 else 0\n        self.deconv1 = nn.ConvTranspose2d(filters[0], input_shape[2], 5, stride=2, padding=2, output_padding=out_pad,\n                                          bias=bias)\n        self.clustering = ClusterlingLayer(num_clusters, num_clusters)\n        # ReLU copies for graph representation in tensorboard\n        self.relu1_1 = copy.deepcopy(self.relu)\n        self.relu2_1 = copy.deepcopy(self.relu)\n        self.relu3_1 = copy.deepcopy(self.relu)\n        self.relu4_1 = copy.deepcopy(self.relu)\n        self.relu5_1 = copy.deepcopy(self.relu)\n        self.relu1_2 = copy.deepcopy(self.relu)\n        self.relu2_2 = copy.deepcopy(self.relu)\n        self.relu3_2 = copy.deepcopy(self.relu)\n        self.relu4_2 = copy.deepcopy(self.relu)\n        self.relu5_2 = copy.deepcopy(self.relu)\n        self.sig = nn.Sigmoid()\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu1_1(x)\n        x = self.conv2(x)\n        x = self.relu2_1(x)\n        x = self.conv3(x)\n        x = self.relu3_1(x)\n        x = self.conv4(x)\n        x = self.relu4_1(x)\n        x = self.conv5(x)\n        if self.activations:\n            x = self.sig(x)\n        else:\n            x = self.relu5_1(x)\n        x = x.view(x.size(0), -1)\n        x = self.embedding(x)\n        extra_out = x\n        clustering_out = self.clustering(x)\n        x = self.deembedding(x)\n        x = self.relu4_2(x)\n        x = x.view(x.size(0), self.filters[4], ((self.input_shape[0]//2//2//2//2-1) // 2), ((self.input_shape[0]//2//2//2//2-1) // 2))\n        x = self.deconv5(x)\n        x = self.relu4_2(x)\n        x = self.deconv4(x)\n        x = self.relu3_2(x)\n        x = self.deconv3(x)\n        x = self.relu2_2(x)\n        x = self.deconv2(x)\n        x = self.relu1_2(x)\n        x = self.deconv1(x)\n        if self.activations:\n            x = self.tanh(x)\n        return x, clustering_out, extra_out\n\n\n# Convolutional autoencoder with 5 convolutional blocks (BN version)\nclass CAE_bn5(nn.Module):\n    def __init__(self, input_shape=[128,128,3], num_clusters=10, filters=[32, 64, 128, 256, 512], leaky=True, neg_slope=0.01, activations=False, bias=True):\n        super(CAE_bn5, self).__init__()\n        self.activations = activations\n        self.pretrained = False\n        self.num_clusters = num_clusters\n        self.input_shape = input_shape\n        self.filters = filters\n        self.relu = nn.ReLU(inplace=False)\n        if leaky:\n            self.relu = nn.LeakyReLU(negative_slope=neg_slope)\n        else:\n            self.relu = nn.ReLU(inplace=False)\n\n        self.conv1 = nn.Conv2d(input_shape[2], filters[0], 5, stride=2, padding=2, bias=bias)\n        self.bn1_1 = nn.BatchNorm2d(filters[0])\n        self.conv2 = nn.Conv2d(filters[0], filters[1], 5, stride=2, padding=2, bias=bias)\n        self.bn2_1 = nn.BatchNorm2d(filters[1])\n        self.conv3 = nn.Conv2d(filters[1], filters[2], 5, stride=2, padding=2, bias=bias)\n        self.bn3_1 = nn.BatchNorm2d(filters[2])\n        self.conv4 = nn.Conv2d(filters[2], filters[3], 5, stride=2, padding=2, bias=bias)\n        self.bn4_1 = nn.BatchNorm2d(filters[3])\n        self.conv5 = nn.Conv2d(filters[3], filters[4], 3, stride=2, padding=0, bias=bias)\n\n        lin_features_len = ((input_shape[0] // 2 // 2 // 2 // 2 - 1) // 2) * (\n                    (input_shape[0] // 2 // 2 // 2 // 2 - 1) // 2) * filters[4]\n        self.embedding = nn.Linear(lin_features_len, num_clusters, bias=bias)\n        self.deembedding = nn.Linear(num_clusters, lin_features_len, bias=bias)\n        out_pad = 1 if input_shape[0] // 2 // 2 // 2 // 2 % 2 == 0 else 0\n        self.deconv5 = nn.ConvTranspose2d(filters[4], filters[3], 3, stride=2, padding=0, output_padding=out_pad,\n                                          bias=bias)\n        self.bn5_2 = nn.BatchNorm2d(filters[3])\n        out_pad = 1 if input_shape[0] // 2 // 2 // 2 % 2 == 0 else 0\n        self.deconv4 = nn.ConvTranspose2d(filters[3], filters[2], 5, stride=2, padding=2, output_padding=out_pad,\n                                          bias=bias)\n        self.bn4_2 = nn.BatchNorm2d(filters[2])\n        out_pad = 1 if input_shape[0] // 2 // 2 % 2 == 0 else 0\n        self.deconv3 = nn.ConvTranspose2d(filters[2], filters[1], 5, stride=2, padding=2, output_padding=out_pad,\n                                          bias=bias)\n        self.bn3_2 = nn.BatchNorm2d(filters[1])\n        out_pad = 1 if input_shape[0] // 2 % 2 == 0 else 0\n        self.deconv2 = nn.ConvTranspose2d(filters[1], filters[0], 5, stride=2, padding=2, output_padding=out_pad,\n                                          bias=bias)\n        self.bn2_2 = nn.BatchNorm2d(filters[0])\n        out_pad = 1 if input_shape[0] % 2 == 0 else 0\n        self.deconv1 = nn.ConvTranspose2d(filters[0], input_shape[2], 5, stride=2, padding=2, output_padding=out_pad,\n                                          bias=bias)\n        self.clustering = ClusterlingLayer(num_clusters, num_clusters)\n        # ReLU copies for graph representation in tensorboard\n        self.relu1_1 = copy.deepcopy(self.relu)\n        self.relu2_1 = copy.deepcopy(self.relu)\n        self.relu3_1 = copy.deepcopy(self.relu)\n        self.relu4_1 = copy.deepcopy(self.relu)\n        self.relu5_1 = copy.deepcopy(self.relu)\n        self.relu1_2 = copy.deepcopy(self.relu)\n        self.relu2_2 = copy.deepcopy(self.relu)\n        self.relu3_2 = copy.deepcopy(self.relu)\n        self.relu4_2 = copy.deepcopy(self.relu)\n        self.relu5_2 = copy.deepcopy(self.relu)\n        self.sig = nn.Sigmoid()\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu1_1(x)\n        x = self.bn1_1(x)\n        x = self.conv2(x)\n        x = self.relu2_1(x)\n        x = self.bn2_1(x)\n        x = self.conv3(x)\n        x = self.relu3_1(x)\n        x = self.bn3_1(x)\n        x = self.conv4(x)\n        x = self.relu4_1(x)\n        x = self.bn4_1(x)\n        x = self.conv5(x)\n        if self.activations:\n            x = self.sig(x)\n        else:\n            x = self.relu5_1(x)\n        x = x.view(x.size(0), -1)\n        x = self.embedding(x)\n        extra_out = x\n        clustering_out = self.clustering(x)\n        x = self.deembedding(x)\n        x = self.relu5_2(x)\n        x = x.view(x.size(0), self.filters[4], ((self.input_shape[0]//2//2//2//2-1) // 2), ((self.input_shape[0]//2//2//2//2-1) // 2))\n        x = self.deconv5(x)\n        x = self.relu4_2(x)\n        x = self.bn5_2(x)\n        x = self.deconv4(x)\n        x = self.relu3_2(x)\n        x = self.bn4_2(x)\n        x = self.deconv3(x)\n        x = self.relu2_2(x)\n        x = self.bn3_2(x)\n        x = self.deconv2(x)\n        x = self.relu1_2(x)\n        x = self.bn2_2(x)\n        x = self.deconv1(x)\n        if self.activations:\n            x = self.tanh(x)\n        return x, clustering_out, extra_out\n\nclass LossNet(nn.Module):\n    def __init__(self, feature_sizes=[14, 7], num_channels=[32, 64 ], interm_dim=10):\n        super(LossNet, self).__init__()\n\n        self.GAP1 = nn.AvgPool2d(feature_sizes[0])\n        self.GAP2 = nn.AvgPool2d(feature_sizes[1])\n        \n\n        self.FC1 = nn.Linear(num_channels[0], interm_dim)\n        self.FC2 = nn.Linear(num_channels[1], interm_dim)\n        \n\n\n        self.linear = nn.Linear(2 * interm_dim, 1)\n\n    def forward(self, features):\n        out1 = self.GAP1(features[0])\n        out1 = out1.view(out1.size(0), -1)\n        out1 = F.relu(self.FC1(out1))\n\n        out2 = self.GAP2(features[1])\n        out2 = out2.view(out2.size(0), -1)\n        out2 = F.relu(self.FC2(out2))\n        \n        \n\n        out = self.linear(torch.cat((out1, out2), 1))\n\n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, dataloader, criteria, optimizers, schedulers, num_epochs, params):\n\n    # Note the time\n    since = time.time()\n    df = pd.DataFrame(columns=['epoch', 'train_loss'])\n    df.to_csv(\"1.csv\", index=False)\n    \n    df = pd.DataFrame(columns=['epoch', 'nmi','acc'])\n    df.to_csv(\"2.csv\", index=False)\n\n    df = pd.DataFrame(columns=['epoch', 'acc', 'nmi'])\n    df.to_csv(\"3.csv\", index=False)\n    \n    # Unpack parameters\n    writer = params['writer']\n    if writer is not None: board = True\n    txt_file = params['txt_file']\n    pretrained = params['model_files'][1]\n    pretrain = params['pretrain']\n    print_freq = params['print_freq']\n    dataset_size = params['dataset_size']\n    device = params['device']\n    batch = params['batch']\n    pretrain_epochs = params['pretrain_epochs']\n    gamma = params['gamma']\n    update_interval = params['update_interval']\n    tol = params['tol']\n\n    dl = dataloader\n    loss_module = LossNet().to(device)\n    optimizer1 = optim.Adam(loss_module.parameters())\n    # Pretrain or load weights\n    if pretrain:\n        while True:\n            pretrained_model = pretraining(model, copy.deepcopy(dl), criteria[0], optimizers[1], schedulers[1], pretrain_epochs, params)\n            if pretrained_model:\n                break\n            else:\n                for layer in model.children():\n                    if hasattr(layer, 'reset_parameters'):\n                        layer.reset_parameters()\n        model = pretrained_model\n    else:\n        try:\n            model.load_state_dict(torch.load(pretrained))\n            print_both(txt_file, 'Pretrained weights loaded from file: ' + str(pretrained))\n        except:\n            print(\"Couldn't load pretrained weights\")\n\n    # Initialise clusters\n    print_both(txt_file, '\\nInitializing cluster centers based on K-means')\n    kmeans(model, copy.deepcopy(dl), params)\n\n    print_both(txt_file, '\\nBegin clusters training')\n\n    # Prep variables for weights and accuracy of the best model\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss = 10000.0\n\n    # Initial target distribution\n    print_both(txt_file, '\\nUpdating target distribution')\n    output_distribution, labels, preds_prev = calculate_predictions(model, copy.deepcopy(dl), params, 0.000001)\n    target_distribution = target(output_distribution)\n    nmi = metrics.nmi(labels, preds_prev)\n    ari = metrics.ari(labels, preds_prev)\n    acc = metrics.acc(labels, preds_prev)\n    print_both(txt_file,\n                     'NMI: {0:.5f}\\tARI: {1:.5f}\\tAcc {2:.5f}\\n'.format(nmi, ari, acc))\n\n    if board:\n        niter = 0\n        writer.add_scalar('/NMI', nmi, niter)\n        writer.add_scalar('/ARI', ari, niter)\n        writer.add_scalar('/Acc', acc, niter)\n\n    update_iter = 1\n    finished = False\n     \n    # Go through all epochs\n    l = 1\n    for epoch in range(num_epochs):\n   \n        print_both(txt_file, 'Epoch {}/{}'.format(epoch + 1, num_epochs))\n        print_both(txt_file,  '-' * 10)\n\n        schedulers[0].step()\n        model.train(True)  # Set model to training mode\n\n        running_loss = 0.0\n        running_loss_rec = 0.0\n        running_loss_clust = 0.0\n\n        # Keep the batch number for inter-phase statistics\n        batch_num = 1\n        img_counter = 0\n\n        # Iterate over data.\n        l = l *0.000001\n        for data in dataloader:\n            # Get the inputs and labels\n            inputs, _ = data\n\n            inputs = inputs.to(device)\n            \n            label = []\n            \n\n            # data = data.reshape(-1, 28*28)\n            data1 = inputs\n            output, feature1, feature2,fa = model(inputs)\n\n            t = 0\n\n            for i in range(len(feature1)):\n                if (feature1[i].max() - find_max(feature1[i])) > l:\n                    label.append(_[i])\n\n                else:\n\n                    data1 = del_tensor_ele(data1, i - t)\n                    t += 1\n\n            labes = np.array(label)\n            label = torch.tensor([item for item in label])\n            # Uptade target distribution, chack and print performance\n            if (batch_num - 1) % update_interval == 0 and not (batch_num == 1 and epoch == 0):\n                print_both(txt_file, '\\nUpdating target distribution:')\n                output_distribution, labels, preds = calculate_predictions(model, dataloader, params, l)\n                target_distribution = target(output_distribution)\n                nmi = metrics.nmi(labels, preds)\n                ari = metrics.ari(labels, preds)\n                acc = metrics.acc(labels, preds)\n                \n                \n                \n                print_both(txt_file,\n                                 'NMI: {0:.5f}\\tARI: {1:.5f}\\tAcc {2:.5f}\\t'.format(nmi, ari, acc))\n                if board:\n                    niter = update_iter\n                    writer.add_scalar('/NMI', nmi, niter)\n                    writer.add_scalar('/ARI', ari, niter)\n                    writer.add_scalar('/Acc', acc, niter)\n                    update_iter += 1\n\n                # check stop criterion\n                delta_label = np.sum(preds != preds_prev).astype(np.float32) / label.shape[0]\n                preds_prev = np.copy(preds)\n                if delta_label < tol:\n                    print_both(txt_file, 'Label divergence ' + str(delta_label) + '< tol ' + str(tol))\n                    print_both(txt_file, 'Reached tolerance threshold. Stopping training.')\n                    finished = True\n                    break\n\n            tar_dist = target_distribution[((batch_num - 1) * label.shape[0]):(batch_num*label.shape[0]), :]\n            tar_dist = torch.from_numpy(tar_dist).to(device)\n            # print(tar_dist)\n\n            # zero the parameter gradients\n            optimizers[0].zero_grad()\n            optimizer1.zero_grad()\n            # Calculate losses and backpropagate\n            with torch.set_grad_enabled(True):\n                uncertainty = get_uncertainty(model, loss_module, data1)\n\n                # Index in ascending order\n                # #按升序索引\n                arg = np.argsort(uncertainty)\n                target2 = []\n                data2 = []\n                \n                # Update the labeled dataset and the unlabeled dataset, respectively\n                # 更新标记的数据集\n                data2 += list(torch.tensor(data1)[arg][:250].cpu().numpy())\n                data2 = torch.tensor(data2).to(device)\n                target2 += list(torch.tensor(tar_dist)[arg][:250].cpu().numpy())\n                target2 = np.argmax(target2, axis=1)\n                target2 = torch.tensor(target2).to(device)\n                \n                outputs, clusters, _, feature3 = model(data2)\n                \n                if epoch > 200:\n            # After 200 epochs, stop the gradient from the loss prediction module propagated to the target model.\n                     feature3[0] = feature3[0].detach()\n                     feature3[1] = feature3[1].detach()\n            \n\n                loss_pred = loss_module(feature3)\n\n                loss_target = nn.functional.cross_entropy(clusters, target2, reduction='none')\n\n\n\n                loss2 = LossPredLoss(loss_pred, torch.log(loss_target))\n                # 损失模块损失\n                loss_rec = criteria[0](outputs, data2)\n                loss_clust = 0.01*torch.sum(loss_target) / loss_target.size(0)  # target loss 求平均\n                loss = loss_rec + 0.5*loss_clust + loss2*0.5\n                loss.backward()\n                optimizers[0].step()\n                optimizer1.step()\n \n            # For keeping statistics\n            running_loss += loss.item() * data2.size(0)\n            running_loss_rec += loss_rec.item() * data2.size(0)\n            running_loss_clust += loss_clust.item() * data2.size(0)\n            \n            list1=[epoch,loss.item()]\n            \n            data = pd.DataFrame([list1])\n\n            data.to_csv('1.csv', mode='a', header=False, index=False)\n            \n\n\n         \n            \n            \n            # Some current stats\n            loss_batch = loss.item()\n            loss_batch_rec = loss_rec.item()\n            loss_batch_clust = loss_clust.item()\n            loss_accum = running_loss / ((batch_num - 1) * target2.shape[0] + data2.size(0))\n            loss_accum_rec = running_loss_rec / ((batch_num - 1) * target2.shape[0] + data2.size(0))\n            loss_accum_clust = running_loss_clust / ((batch_num - 1) * target2.shape[0] + data2.size(0))\n\n            if batch_num % print_freq == 0:\n                print_both(txt_file, 'Epoch: [{0}][{1}/{2}]\\t'\n                                           'Loss {3:.4f} ({4:.4f})\\t'\n                                           'Loss_recovery {5:.4f} ({6:.4f})\\t'\n                                           'Loss clustering {7:.4f} ({8:.4f})\\t'.format(epoch + 1, batch_num,\n                                                                                        len(dataloader),\n                                                                                        loss_batch,\n                                                                                        loss_accum, loss_batch_rec,\n                                                                                        loss_accum_rec,\n                                                                                        loss_batch_clust,\n                                                                                        loss_accum_clust))\n                if board:\n                    niter = epoch * len(dataloader) + batch_num\n                    writer.add_scalar('/Loss', loss_accum, niter)\n                    writer.add_scalar('/Loss_recovery', loss_accum_rec, niter)\n                    writer.add_scalar('/Loss_clustering', loss_accum_clust, niter)\n            batch_num = batch_num + 1\n\n            # Print image to tensorboard\n            if batch_num == len(dataloader) and (epoch+1) % 5:\n                inp = tensor2img(data1)\n                out = tensor2img(outputs)\n                if board:\n                    img = np.concatenate((inp, out), axis=1)\n                    writer.add_image('Clustering/Epoch_' + str(epoch + 1).zfill(3) + '/Sample_' + str(img_counter).zfill(2), img)\n                    img_counter += 1\n\n        if finished: break\n\n        epoch_loss = running_loss / dataset_size\n        epoch_loss_rec = running_loss_rec / dataset_size\n        epoch_loss_clust = running_loss_clust / dataset_size\n\n        if board:\n            writer.add_scalar('/Loss' + '/Epoch', epoch_loss, epoch + 1)\n            writer.add_scalar('/Loss_rec' + '/Epoch', epoch_loss_rec, epoch + 1)\n            writer.add_scalar('/Loss_clust' + '/Epoch', epoch_loss_clust, epoch + 1)\n        \n        print_both(txt_file, 'Loss_recovery: {1:.4f}\\tLoss_clustering: {2:.4f}'.format(epoch_loss_rec,epoch_loss_clust))\n\n        # If wanted to do some criterium in the future (for now useless)\n        if epoch_loss < best_loss or epoch_loss >= best_loss:\n            best_loss = epoch_loss\n            best_model_wts = copy.deepcopy(model.state_dict())\n\n        print_both(txt_file, '')\n        \n        list2=[epoch,nmi,acc]\n        data4 = pd.DataFrame([list2])\n        data4.to_csv('2.csv', mode='a', header=False, index=False)\n        \n        if (epoch+1)%20==0:\n            list3=[epoch, acc, nmi]\n            data_dd1 = pd.DataFrame([list3])\n            data_dd1.to_csv('3.csv', mode='a', header=False, index=False)\n\n    time_elapsed = time.time() - since\n    print_both(txt_file, 'Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model\n\n\n# Pretraining function for recovery loss only\ndef pretraining(model, dataloader, criterion, optimizer, scheduler, num_epochs, params):\n    # Note the time\n    since = time.time()\n\n    # Unpack parameters\n    writer = params['writer']\n    if writer is not None: board = True\n    txt_file = params['txt_file']\n    pretrained = params['model_files'][1]\n    print_freq = params['print_freq']\n    dataset_size = params['dataset_size']\n    device = params['device']\n    batch = params['batch']\n\n    # Prep variables for weights and accuracy of the best model\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss = 10000.0\n\n    # Go through all epochs\n    for epoch in range(num_epochs):\n        print_both(txt_file, 'Pretraining:\\tEpoch {}/{}'.format(epoch + 1, num_epochs))\n        print_both(txt_file, '-' * 10)\n\n        scheduler.step()\n        model.train(True)  # Set model to training mode\n\n        running_loss = 0.0\n\n        # Keep the batch number for inter-phase statistics\n        batch_num = 1\n        # Images to show\n        img_counter = 0\n\n        # Iterate over data.\n        for data in dataloader:\n            # Get the inputs and labels\n            inputs, _ = data\n            inputs = inputs.to(device)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            with torch.set_grad_enabled(True):\n                outputs, _, _,_ = model(inputs)\n                loss = criterion(outputs, inputs)\n                loss.backward()\n                optimizer.step()\n\n            # For keeping statistics\n            running_loss += loss.item() * inputs.size(0)\n\n            # Some current stats\n            loss_batch = loss.item()\n            loss_accum = running_loss / ((batch_num - 1) * batch + inputs.size(0))\n\n            if batch_num % print_freq == 0:\n#                 print_both(txt_file, 'Pretraining:\\tEpoch: [{0}][{1}/{2}]\\t'\n#                            'Loss {3:.4f} ({4:.4f})\\t'.format(epoch + 1, batch_num, len(dataloader),\n#                                                              loss_batch,\n#                                                              loss_accum))\n                if board:\n                    niter = epoch * len(dataloader) + batch_num\n                    writer.add_scalar('Pretraining/Loss', loss_accum, niter)\n            batch_num = batch_num + 1\n\n            if batch_num in [len(dataloader), len(dataloader)//2, len(dataloader)//4, 3*len(dataloader)//4]:\n                inp = tensor2img(inputs)\n                out = tensor2img(outputs)\n                if board:\n                    img = np.concatenate((inp, out), axis=1)\n                    writer.add_image('Pretraining/Epoch_' + str(epoch + 1).zfill(3) + '/Sample_' + str(img_counter).zfill(2), img)\n                    img_counter += 1\n\n        epoch_loss = running_loss / dataset_size\n        if epoch == 0: first_loss = epoch_loss\n        if epoch == 4 and epoch_loss / first_loss > 1:\n            print_both(txt_file, \"\\nLoss not converging, starting pretraining again\\n\")\n            return False\n\n        if board:\n            writer.add_scalar('Pretraining/Loss' + '/Epoch', epoch_loss, epoch + 1)\n\n        print_both(txt_file, 'Pretraining:\\t Loss: {:.4f}'.format(epoch_loss))\n\n        # If wanted to add some criterium in the future\n        if epoch_loss < best_loss or epoch_loss >= best_loss:\n            best_loss = epoch_loss\n            best_model_wts = copy.deepcopy(model.state_dict())\n\n        print_both(txt_file, '')\n\n    time_elapsed = time.time() - since\n    print_both(txt_file, 'Pretraining complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    model.pretrained = True\n    torch.save(model.state_dict(), pretrained)\n\n    return model\n\n\n# K-means clusters initialisation\ndef kmeans(model, dataloader, params):\n    km = KMeans(n_clusters=model.num_clusters, n_init=20)\n    output_array = None\n    model.eval()\n    # Itarate throught the data and concatenate the latent space representations of images\n    for data in dataloader:\n        inputs, _ = data\n        inputs = inputs.to(params['device'])\n        _, _, outputs,_ = model(inputs)\n\n        if output_array is not None:\n            output_array = np.concatenate((output_array, outputs.cpu().detach().numpy()), 0)\n        else:\n            output_array = outputs.cpu().detach().numpy()\n        # print(output_array.shape)\n        if output_array.shape[0] > 50000: break\n\n    # Perform K-means\n    km.fit_predict(output_array)\n    # Update clustering layer weights\n    weights = torch.from_numpy(km.cluster_centers_)\n    model.clustering.set_weight(weights.to(params['device']))\n    # torch.cuda.empty_cache()\n\n\n# Function forwarding data through network, collecting clustering weight output and returning prediciotns and labels\ndef calculate_predictions(model, dataloader, params, l):\n    output_array = None\n    label_array = None\n    model.eval()\n    for data in dataloader:\n        inputs, labels = data\n        inputs = inputs.to(device)\n        label = []\n        data1 = inputs\n        output, feature1, feature2,_ = model(inputs)\n\n        t = 0\n\n        for i in range(len(feature1)):\n            if (feature1[i].max() - find_max(feature1[i])) > l:\n\n                label.append(labels[i])\n\n            else:\n\n                data1 = del_tensor_ele(data1, i - t)\n                t += 1\n\n        label = np.array(label)\n        label = torch.tensor([item for item in label])\n        \n        data1 = data1.to(params['device'])\n       \n        label = label.to(params['device'])\n        \n        _, outputs, _,_ = model(data1)\n        if output_array is not None:\n            output_array = np.concatenate((output_array, outputs.cpu().detach().numpy()), 0)\n            label_array = np.concatenate((label_array, label.cpu().detach().numpy()), 0)\n        else:\n            output_array = outputs.cpu().detach().numpy()\n            label_array = label.cpu().detach().numpy()\n\n    preds = np.argmax(output_array.data, axis=1)\n    \n    return output_array, label_array, preds\n\n\n# Calculate target distribution\ndef target(out_distr):\n    tar_dist = out_distr ** 2 / np.sum(out_distr, axis=0)\n    tar_dist = np.transpose(np.transpose(tar_dist) / np.sum(tar_dist, axis=1))\n    return tar_dist\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_max(a):\n    a = a.detach().cpu().numpy()\n\n    b = np.zeros(10, dtype=int)\n\n\n    c = np.zeros(10)\n\n    c[0] = a.max()  # 最大值\n    \n    b[0] = np.where(a == c[0])[0][0]\n    # 最大``值位置\n    new_a = np.delete(a, b[0])\n    c[1] = np.max(new_a)  # 次大值\n    return c[1]\n\n\ndef del_tensor_ele(arr,index):\n    arr1 = arr[0:index]\n    arr2 = arr[index+1:]\n    return torch.cat((arr1,arr2),dim=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_uncertainty(models1, models2, data1):\n    models1.eval()\n    models2.eval()\n    uncertainty = torch.tensor([]).to(device)\n\n    with torch.no_grad():\n        inputs = data1.to(device)\n        \n        x, out1, out2, features = models1(inputs)\n        \n        pred_loss = models2(features)\n        \n        pred_loss = pred_loss.view(pred_loss.size(0))\n\n        uncertainty = torch.cat((uncertainty, pred_loss), 0)\n\n    return uncertainty.cpu()\n\ndef LossPredLoss(input, target, margin=1.0,reduction='mean'):  # input 一个shape为[N,C]的Tensor，其中N代表样本个数，C代表类别数目  mean表明对N个样本的loss进行求平均之后返回\n\n    \n    assert input.shape == input.flip(0).shape  # 将数据上下翻转，行序发生颠倒\n    \n\n    input = (input - input.flip(0))[\n            :len(input) // 2]  # [l_1 - l_2B, l_2 - l_2B-1, ... , l_B - l_B+1], where batch_size = 2B\n    target = (target - target.flip(0))[:len(target) // 2]\n    # print(target)\n    target = target.detach()  # 切断target的反向传播\n\n    # sign()该函数的作用就是输出input通过sign函数后的张量，其中sign函数就是符号函数\n    # 将输入target张量每个元素的范围限制到区间 [min,max]，返回结果到一个新张量\n    one = 2 * torch.sign(\n        torch.clamp(target, min=0)) - 1  # 1 operation which is defined by the authors one是一个根据target的数据来得出（-1，1）的矩阵\n    # print(one)\n    if reduction == 'mean':\n        loss = torch.sum(torch.clamp(margin - one * input, min=0))\n        loss = loss / input.size(0)  # Note that the size of input is already halved\n        # print(loss)\n    elif reduction == 'none':\n        loss = torch.clamp(margin - one * input, min=0)\n    else:\n        NotImplementedError()\n\n    return loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean = (0.485, 0.456, 0.406)\nstd = (0.229, 0.224, 0.225)\ninv_normalize = transforms.Normalize(\n    mean=[-0.485 / .229, -0.456 / 0.224, -0.406 / 0.255],\n    std=[1 / 0.229, 1 / 0.224, 1 / 0.255]\n)\n\n\n# Simple tensor to image translation\ndef tensor2img(tensor):\n    img = tensor.cpu().data[0]\n    if img.shape[0] != 1:\n        img = inv_normalize(img)\n    img = torch.clamp(img, 0, 1)\n    return img\n\n\n# Define printing to console and file\ndef print_both(f, text):\n    print(text)\n    f.write(text + '\\n')\n\n\n# Metrics class was copied from DCEC article authors repository (link in README)\nclass metrics:\n    nmi = sklearn.metrics.normalized_mutual_info_score\n    ari = sklearn.metrics.adjusted_rand_score\n\n    @staticmethod\n    def acc(labels_true, labels_pred):\n        labels_true = labels_true.astype(np.int64)\n        assert labels_pred.size == labels_true.size\n        D = max(labels_pred.max(), labels_true.max()) + 1\n        w = np.zeros((D, D), dtype=np.int64)\n        for i in range(labels_pred.size):\n            w[labels_pred[i], labels_true[i]] += 1\n        from scipy.optimize import linear_sum_assignment as linear_assignment\n        ind = linear_assignment(w.max() - w)\n        a = []\n        for (i ,j) in zip(ind[0],ind[1]):\n            a.append(w[i,j])\n\n        return sum(a) * 1.0 / labels_pred.size\n#         return sum([w[i, j] for i, j in ind]) * 1.0 / labels_pred.size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nif __name__ == \"__main__\":\n\n   \n\n    # Translate string entries to bool for parser\n    def str2bool(v):\n        if v.lower() in ('yes', 'true', 't', 'y', '1'):\n            return True\n        elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n            return False\n        else:\n            raise argparse.ArgumentTypeError('Boolean value expected.')\n\n    parser = argparse.ArgumentParser(description='Use DCEC for clustering')\n    parser.add_argument('--mode', default='train_full', choices=['train_full', 'pretrain'], help='mode')\n    parser.add_argument('--tensorboard', default=True, type=bool, help='export training stats to tensorboard')\n    parser.add_argument('--pretrain', default=True, type=str2bool, help='perform autoencoder pretraining')\n    parser.add_argument('--pretrained_net', default=1, help='index or path of pretrained net')\n    parser.add_argument('--net_architecture', default='CAE_bn3', choices=['CAE_3', 'CAE_bn3', 'CAE_4', 'CAE_bn4', 'CAE_5', 'CAE_bn5'], help='network architecture used')\n    parser.add_argument('--dataset', default='MNIST-test',choices=[ 'MNIST-test'],help='custom or prepared dataset')\n    parser.add_argument('--dataset_path', default='data', help='path to dataset')\n    parser.add_argument('--batch_size', default=256, type=int, help='batch size')\n    parser.add_argument('--rate', default=0.0001, type=float, help='learning rate for clustering')\n    parser.add_argument('--rate_pretrain', default=0.001, type=float, help='learning rate for pretraining')\n    parser.add_argument('--weight', default=0.0, type=float, help='weight decay for clustering')\n    parser.add_argument('--weight_pretrain', default=0.0, type=float, help='weight decay for clustering')\n    parser.add_argument('--sched_step', default=200, type=int, help='scheduler steps for rate update')\n    parser.add_argument('--sched_step_pretrain', default=200, type=int,\n                        help='scheduler steps for rate update - pretrain')\n    parser.add_argument('--sched_gamma', default=0.1, type=float, help='scheduler gamma for rate update')\n    parser.add_argument('--sched_gamma_pretrain', default=0.1, type=float,\n                        help='scheduler gamma for rate update - pretrain')\n    parser.add_argument('--epochs', default=100, type=int, help='clustering epochs')\n    parser.add_argument('--epochs_pretrain', default=400, type=int, help='pretraining epochs')\n    parser.add_argument('--printing_frequency', default=10, type=int, help='training stats printing frequency')\n    parser.add_argument('--gamma', default=0.1, type=float, help='clustering loss weight')\n    parser.add_argument('--update_interval', default=80, type=int, help='update interval for target distribution')\n    parser.add_argument('--tol', default=1e-3, type=float, help='stop criterium tolerance')\n    parser.add_argument('--num_clusters', default=10, type=int, help='number of clusters')\n    parser.add_argument('--custom_img_size', default=[128, 128, 3], nargs=3, type=int, help='size of custom images')\n    parser.add_argument('--leaky', default=True, type=str2bool)\n    parser.add_argument('--neg_slope', default=0.01, type=float)\n    parser.add_argument('--activations', default=False, type=str2bool)\n    parser.add_argument('--bias', default=True, type=str2bool)\n    args = parser.parse_known_args()[0]\n    print(args)\n    \n    if args.mode == 'pretrain' and not args.pretrain:\n        print(\"Nothing to do :(\")\n        exit()\n\n    board = args.tensorboard\n    \n\n    # Deal with pretraining option and way of showing network path\n    pretrain = args.pretrain\n    net_is_path = True\n    if not pretrain:\n        try:\n            int(args.pretrained_net)\n            idx = args.pretrained_net\n            net_is_path = False\n        except:\n            pass\n    params = {'pretrain': pretrain}\n\n    # Directories\n    # Create directories structure\n    dirs = ['runs', 'reports', 'nets']\n    list(map(lambda x: os.makedirs(x, exist_ok=True), dirs))\n\n    # Net architecture\n    model_name = args.net_architecture\n    # Indexing (for automated reports saving) - allows to run many trainings and get all the reports collected\n    if pretrain or (not pretrain and net_is_path):\n        reports_list = sorted(os.listdir('reports'), reverse=True)\n        if reports_list:\n            for file in reports_list:\n                # print(file)\n                if fnmatch.fnmatch(file, model_name + '*'):\n                    idx = int(str(file)[-7:-4]) + 1\n                    break\n        try:\n            idx\n        except NameError:\n            idx = 1\n\n    # Base filename\n    name = model_name + '_' + str(idx).zfill(3)\n\n    # Filenames for report and weights\n    name_txt = name + '.txt'\n    name_net = name\n    pretrained = name + '_pretrained.pt'\n\n    # Arrange filenames for report, network weights, pretrained network weights\n    name_txt = os.path.join('reports', name_txt)\n    name_net = os.path.join('nets', name_net)\n    if net_is_path and not pretrain:\n        pretrained = args.pretrained_net\n    else:\n        pretrained = os.path.join('nets', pretrained)\n    if not pretrain and not os.path.isfile(pretrained):\n        print(\"No pretrained weights, try again choosing pretrained network or create new with pretrain=True\")\n\n    model_files = [name_net, pretrained]\n    params['model_files'] = model_files\n\n    # Open file\n    if pretrain:\n        f = open(name_txt, 'w')\n    else:\n        f = open(name_txt, 'a')\n    params['txt_file'] = f\n\n    # Delete tensorboard entry if exist (not to overlap as the charts become unreadable)\n    try:\n        os.system(\"rm -rf runs/\" + name)\n    except:\n        pass\n\n    # Initialize tensorboard writer\n    if board:\n        writer = SummaryWriter('runs/' + name)\n        params['writer'] = writer\n    else:\n        params['writer'] = None\n\n    # Hyperparameters\n\n    # Used dataset\n    dataset = args.dataset\n\n    # Batch size\n    batch = args.batch_size\n    params['batch'] = batch\n    # Number of workers (typically 4*num_of_GPUs)\n    workers = 4\n    # Learning rate\n    rate = args.rate\n    rate_pretrain = args.rate_pretrain\n    # Adam params\n    # Weight decay\n    weight = args.weight\n    weight_pretrain = args.weight_pretrain\n    # Scheduler steps for rate update\n    sched_step = args.sched_step\n    sched_step_pretrain = args.sched_step_pretrain\n    # Scheduler gamma - multiplier for learning rate\n    sched_gamma = args.sched_gamma\n    sched_gamma_pretrain = args.sched_gamma_pretrain\n\n    # Number of epochs\n    epochs = args.epochs\n    pretrain_epochs = args.epochs_pretrain\n    params['pretrain_epochs'] = pretrain_epochs\n\n    # Printing frequency\n    print_freq = args.printing_frequency\n    params['print_freq'] = print_freq\n\n    # Clustering loss weight:\n    gamma = args.gamma\n    params['gamma'] = gamma\n\n    # Update interval for target distribution:\n    update_interval = args.update_interval\n    params['update_interval'] = update_interval\n\n    # Tolerance for label changes:\n    tol = args.tol\n    params['tol'] = tol\n\n    # Number of clusters\n    num_clusters = args.num_clusters\n\n    \n    # Data preparation\n    if dataset == 'MNIST-test':\n        # Uses slightly modified torchvision MNIST class\n        \n        # tmp = \"\\nData preparation\\nReading data from: MNIST train dataset\"\n        # print_both(f, tmp)\n        img_size = [28, 28, 1]\n        # tmp = \"Image size used:\\t{0}x{1}\".format(img_size[0], img_size[1])\n        # print_both(f, tmp)\n\n        dataset = MNIST('../data', full=True, download=True,\n                              transform=transforms.Compose([\n                                                           transforms.ToTensor(),\n                                                           # transforms.Normalize((0.1307,), (0.3081,))\n                                                           ]))\n\n        dataloader = torch.utils.data.DataLoader(dataset,\n            batch_size=batch, shuffle=False, num_workers=workers)\n\n        dataset_size = len(dataset)\n        # tmp = \"Training set size:\\t\" + str(dataset_size)\n        # print_both(f, tmp)\n\n   \n    else:\n        # Data folder\n        data_dir = args.dataset_path\n        tmp = \"\\nData preparation\\nReading data from:\\t./\" + data_dir\n        print_both(f, tmp)\n\n        # Image size\n        custom_size = math.nan\n        custom_size = args.custom_img_size\n        if isinstance(custom_size, list):\n            img_size = custom_size\n\n        tmp = \"Image size used:\\t{0}x{1}\".format(img_size[0], img_size[1])\n        print_both(f, tmp)\n\n        # Transformations\n        data_transforms = transforms.Compose([\n                transforms.Resize(img_size[0:2]),\n                # transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n            ])\n\n        # Read data from selected folder and apply transformations\n        image_dataset = datasets.ImageFolder(data_dir, data_transforms)\n        # Prepare data for network: schuffle and arrange batches\n        dataloader = torch.utils.data.DataLoader(image_dataset, batch_size=batch,\n                                                      shuffle=False, num_workers=workers)\n\n        # Size of data sets\n        dataset_size = len(image_dataset)\n        tmp = \"Training set size:\\t\" + str(dataset_size)\n        print_both(f, tmp)\n\n    params['dataset_size'] = dataset_size\n\n    # GPU check\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    tmp = \"\\nPerforming calculations on:\\t\" + str(device)\n    print_both(f, tmp + '\\n')\n    params['device'] = device\n\n    # Evaluate the proper model\n    to_eval =  model_name + \"(img_size, num_clusters=num_clusters, leaky = args.leaky, neg_slope = args.neg_slope)\"\n    model = eval(to_eval)\n\n    # Tensorboard model representation\n    # if board:\n    #     writer.add_graph(model, torch.autograd.Variable(torch.Tensor(batch, img_size[2], img_size[0], img_size[1])))\n\n    model = model.to(device)\n    # Reconstruction loss\n    criterion_1 = nn.MSELoss(size_average=True)\n    # Clustering loss\n    criterion_2 = nn.KLDivLoss(size_average=False)\n\n    criteria = [criterion_1, criterion_2]\n\n    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=rate, weight_decay=weight)\n\n    optimizer_pretrain = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=rate_pretrain, weight_decay=weight_pretrain)\n\n    optimizers = [optimizer, optimizer_pretrain]\n\n    scheduler = lr_scheduler.StepLR(optimizer, step_size=sched_step, gamma=sched_gamma)\n    scheduler_pretrain = lr_scheduler.StepLR(optimizer_pretrain, step_size=sched_step_pretrain, gamma=sched_gamma_pretrain)\n\n    schedulers = [scheduler, scheduler_pretrain]\n\n    if args.mode == 'train_full':\n        model = train_model(model, dataloader, criteria, optimizers, schedulers, epochs, params)\n    elif args.mode == 'pretrain':\n        model = pretraining(model, dataloader, criteria[0], optimizers[1], schedulers[1], epochs, params)\n\n    # Save final model\n    torch.save(model.state_dict(), name_net + '.pt')\n\n    # Close files\n    f.close()\n    if board:\n        writer.close()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}