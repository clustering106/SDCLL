# LDCEC1
LEARNING LOSS WITH CLUSTER
Deep clustering gains superior performance than convertional clustering, and has been widely applied in various fields. Since it is difficult to determine the classification of data located near the decision boundary, those hard or noisy samples will confuse or even mislead the training process of deep neural networks, and finally affect the clustering performance. To address this problem, we propose a novel self-paced deep clustering method, which gradually increase the number of samples input into the network from easy to difficult. Specifically, We attache a loss prediction module to the convolutional neural network to judge the difficulty of samples. The module is robust as it depends on the input contents, rather than statistically estimating uncertainty from outputs. The most informative data will be selected for training model. Therefore,  the network can converge stably and reach a good optimal solution. We finally consider the reconstruction error, clustering loss and loss prediction error to construct the loss of the model. The experiments on on four image datasets demonstrate that our method can outperform state-of-the-art work.
